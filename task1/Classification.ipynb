{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4502ebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31faa5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score,train_test_split,GridSearchCV,KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38e31229",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_raw = pd.read_csv(\"X_test_new_2.csv\", index_col=0).values\n",
    "X_train_raw = pd.read_csv(\"X_train_new_2.csv\", index_col=0).values\n",
    "y_train_raw = pd.read_csv(\"y_train.csv\", index_col=0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d195fa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler,RobustScaler,PowerTransformer\n",
    "def standardize_data(X_train,X_test) :\n",
    "    r_train = RobustScaler()\n",
    "    r_test = RobustScaler()\n",
    "    \n",
    "    p_train = PowerTransformer()\n",
    "    r_train.fit(X_train)\n",
    "    r_test.fit(X_test)\n",
    "    \n",
    "    scaled_train_data = r_train.transform(X_train)\n",
    "    scaled_test_data = r_train.transform(X_test)\n",
    "    \n",
    "    return scaled_train_data,scaled_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6410c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1,X_test_1 = standardize_data(X_train_raw,X_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6f87929",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100,\n",
    "                             criterion='gini', \n",
    "                             max_depth=None, \n",
    "                             min_samples_split=2,\n",
    "                             min_samples_leaf=1, \n",
    "                             min_weight_fraction_leaf=0.0,\n",
    "                             max_features='sqrt', \n",
    "                             max_leaf_nodes=None,\n",
    "                             min_impurity_decrease=0.0, \n",
    "                             bootstrap=True, \n",
    "                             oob_score=False,\n",
    "                             n_jobs=None, \n",
    "                             random_state=None, \n",
    "                             verbose=0, \n",
    "                             warm_start=False,\n",
    "                             class_weight=None, \n",
    "                             ccp_alpha=0.0, \n",
    "                             max_samples=None)\n",
    "clf.fit(X_train_1,y_train_raw.ravel())\n",
    "pred = clf.predict(X_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a714a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = np.hstack([np.arange(0, len(pred)).reshape(-1,1), pred.reshape(-1,1)]) \n",
    "submission_pd = pd.DataFrame(submission, columns=['id','y'])\n",
    "submission_pd.to_csv('submission.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a28d4a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "def outlier_detection(X_train, y_train):\n",
    "    clf = IsolationForest(max_samples=100, random_state = 4)\n",
    "    preds = clf.fit_predict(X_train)\n",
    "    X_train_1 = X_train[preds==1]\n",
    "    y_train_1 = y_train[preds==1]\n",
    "    return X_train_1, y_train_1\n",
    "\n",
    "X_train_2,y_train_2= outlier_detection(X_train_1, y_train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "35bf8274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5117, 493)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6fb0393e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4552, 493)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17da255d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>483</th>\n",
       "      <th>484</th>\n",
       "      <th>485</th>\n",
       "      <th>486</th>\n",
       "      <th>487</th>\n",
       "      <th>488</th>\n",
       "      <th>489</th>\n",
       "      <th>490</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.437283</td>\n",
       "      <td>11.324702</td>\n",
       "      <td>0.119035</td>\n",
       "      <td>-0.827586</td>\n",
       "      <td>-0.423913</td>\n",
       "      <td>-0.063233</td>\n",
       "      <td>-0.030303</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>-0.634277</td>\n",
       "      <td>-0.592320</td>\n",
       "      <td>...</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.061820</td>\n",
       "      <td>-0.259259</td>\n",
       "      <td>0.902666</td>\n",
       "      <td>0.300817</td>\n",
       "      <td>1.741074</td>\n",
       "      <td>0.038550</td>\n",
       "      <td>-0.012644</td>\n",
       "      <td>0.893020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.165243</td>\n",
       "      <td>0.380454</td>\n",
       "      <td>0.166345</td>\n",
       "      <td>0.120690</td>\n",
       "      <td>-0.413043</td>\n",
       "      <td>-0.201605</td>\n",
       "      <td>-0.196970</td>\n",
       "      <td>-0.198667</td>\n",
       "      <td>-0.246757</td>\n",
       "      <td>-0.207562</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.95</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>0.033219</td>\n",
       "      <td>-0.074074</td>\n",
       "      <td>0.615675</td>\n",
       "      <td>0.295694</td>\n",
       "      <td>0.007824</td>\n",
       "      <td>0.138730</td>\n",
       "      <td>0.158757</td>\n",
       "      <td>0.023448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.138774</td>\n",
       "      <td>-0.281788</td>\n",
       "      <td>-0.267372</td>\n",
       "      <td>0.491379</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.331535</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>-0.259823</td>\n",
       "      <td>-0.389069</td>\n",
       "      <td>-0.292726</td>\n",
       "      <td>...</td>\n",
       "      <td>0.55</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>0.275916</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>-0.416429</td>\n",
       "      <td>-0.218770</td>\n",
       "      <td>-0.418872</td>\n",
       "      <td>-0.371190</td>\n",
       "      <td>-0.303386</td>\n",
       "      <td>-0.199231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.902878</td>\n",
       "      <td>0.702110</td>\n",
       "      <td>3.895225</td>\n",
       "      <td>-0.801724</td>\n",
       "      <td>-0.989130</td>\n",
       "      <td>-1.957763</td>\n",
       "      <td>-2.060606</td>\n",
       "      <td>0.051827</td>\n",
       "      <td>2.314668</td>\n",
       "      <td>2.302418</td>\n",
       "      <td>...</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>1.037616</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.333900</td>\n",
       "      <td>3.211963</td>\n",
       "      <td>2.412509</td>\n",
       "      <td>3.955306</td>\n",
       "      <td>4.169896</td>\n",
       "      <td>1.088846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.531289</td>\n",
       "      <td>0.199302</td>\n",
       "      <td>1.477044</td>\n",
       "      <td>-1.103448</td>\n",
       "      <td>-0.804348</td>\n",
       "      <td>-1.073215</td>\n",
       "      <td>-1.075758</td>\n",
       "      <td>0.096700</td>\n",
       "      <td>-0.596560</td>\n",
       "      <td>-0.785713</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.421104</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>-0.134101</td>\n",
       "      <td>1.079097</td>\n",
       "      <td>0.658543</td>\n",
       "      <td>1.323777</td>\n",
       "      <td>1.452337</td>\n",
       "      <td>0.297830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.099861</td>\n",
       "      <td>-0.686025</td>\n",
       "      <td>-0.788633</td>\n",
       "      <td>-0.189655</td>\n",
       "      <td>3.228261</td>\n",
       "      <td>0.993055</td>\n",
       "      <td>0.886364</td>\n",
       "      <td>2.266433</td>\n",
       "      <td>0.258509</td>\n",
       "      <td>0.242827</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.601387</td>\n",
       "      <td>-0.259259</td>\n",
       "      <td>-0.305168</td>\n",
       "      <td>-1.045606</td>\n",
       "      <td>-0.160233</td>\n",
       "      <td>-0.429439</td>\n",
       "      <td>-0.312671</td>\n",
       "      <td>0.662753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.197197</td>\n",
       "      <td>0.870297</td>\n",
       "      <td>0.606443</td>\n",
       "      <td>0.008621</td>\n",
       "      <td>-0.706522</td>\n",
       "      <td>-0.534772</td>\n",
       "      <td>-0.545455</td>\n",
       "      <td>-0.300713</td>\n",
       "      <td>-0.400708</td>\n",
       "      <td>-0.415200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.688276</td>\n",
       "      <td>1.222222</td>\n",
       "      <td>0.496591</td>\n",
       "      <td>0.820696</td>\n",
       "      <td>0.185862</td>\n",
       "      <td>0.531539</td>\n",
       "      <td>0.602661</td>\n",
       "      <td>-0.064510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.591597</td>\n",
       "      <td>0.287134</td>\n",
       "      <td>1.105650</td>\n",
       "      <td>-0.620690</td>\n",
       "      <td>0.978261</td>\n",
       "      <td>-0.842992</td>\n",
       "      <td>-0.992424</td>\n",
       "      <td>1.043028</td>\n",
       "      <td>0.363843</td>\n",
       "      <td>0.286049</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.801445</td>\n",
       "      <td>1.074074</td>\n",
       "      <td>0.572222</td>\n",
       "      <td>-0.397379</td>\n",
       "      <td>1.411587</td>\n",
       "      <td>1.301945</td>\n",
       "      <td>1.362036</td>\n",
       "      <td>1.576827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.905857</td>\n",
       "      <td>12.291612</td>\n",
       "      <td>-0.212686</td>\n",
       "      <td>-0.456897</td>\n",
       "      <td>1.728261</td>\n",
       "      <td>0.287679</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.714874</td>\n",
       "      <td>-0.013373</td>\n",
       "      <td>-0.069576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>-0.239253</td>\n",
       "      <td>-0.259259</td>\n",
       "      <td>0.136953</td>\n",
       "      <td>-0.980137</td>\n",
       "      <td>0.605938</td>\n",
       "      <td>0.053974</td>\n",
       "      <td>0.210664</td>\n",
       "      <td>1.122311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.314776</td>\n",
       "      <td>-14.684118</td>\n",
       "      <td>-1.049204</td>\n",
       "      <td>1.137931</td>\n",
       "      <td>0.771739</td>\n",
       "      <td>1.499147</td>\n",
       "      <td>1.409091</td>\n",
       "      <td>-0.245192</td>\n",
       "      <td>-0.151860</td>\n",
       "      <td>0.444670</td>\n",
       "      <td>...</td>\n",
       "      <td>0.40</td>\n",
       "      <td>-1.035714</td>\n",
       "      <td>-0.146556</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>-0.561316</td>\n",
       "      <td>-0.916312</td>\n",
       "      <td>-0.906998</td>\n",
       "      <td>-1.126371</td>\n",
       "      <td>-1.054905</td>\n",
       "      <td>-0.393775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1.676714</td>\n",
       "      <td>-0.064926</td>\n",
       "      <td>-0.780893</td>\n",
       "      <td>0.232759</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.913080</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.666502</td>\n",
       "      <td>0.871881</td>\n",
       "      <td>0.880261</td>\n",
       "      <td>...</td>\n",
       "      <td>0.85</td>\n",
       "      <td>-0.714286</td>\n",
       "      <td>0.188794</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>-0.785849</td>\n",
       "      <td>-0.751357</td>\n",
       "      <td>-0.525453</td>\n",
       "      <td>-0.734687</td>\n",
       "      <td>-0.634623</td>\n",
       "      <td>-0.074486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.032910</td>\n",
       "      <td>0.036412</td>\n",
       "      <td>-0.100851</td>\n",
       "      <td>0.293103</td>\n",
       "      <td>-0.141304</td>\n",
       "      <td>0.118567</td>\n",
       "      <td>0.075758</td>\n",
       "      <td>-0.121338</td>\n",
       "      <td>1.488768</td>\n",
       "      <td>1.412093</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.90</td>\n",
       "      <td>-0.678571</td>\n",
       "      <td>-0.779983</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.149964</td>\n",
       "      <td>0.026165</td>\n",
       "      <td>-0.289961</td>\n",
       "      <td>-0.178279</td>\n",
       "      <td>-0.084861</td>\n",
       "      <td>-0.200868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-1.313436</td>\n",
       "      <td>-0.130102</td>\n",
       "      <td>-0.886674</td>\n",
       "      <td>0.974138</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.157298</td>\n",
       "      <td>1.106061</td>\n",
       "      <td>-0.313414</td>\n",
       "      <td>-0.116011</td>\n",
       "      <td>-0.082338</td>\n",
       "      <td>...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.178571</td>\n",
       "      <td>-0.051275</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>-0.454634</td>\n",
       "      <td>-0.746975</td>\n",
       "      <td>-0.763399</td>\n",
       "      <td>-0.942918</td>\n",
       "      <td>-0.877118</td>\n",
       "      <td>-0.312454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.789213</td>\n",
       "      <td>3.439092</td>\n",
       "      <td>-0.302727</td>\n",
       "      <td>0.439655</td>\n",
       "      <td>-0.195652</td>\n",
       "      <td>0.269697</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>-0.335341</td>\n",
       "      <td>1.139460</td>\n",
       "      <td>1.423966</td>\n",
       "      <td>...</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>1.211476</td>\n",
       "      <td>1.259259</td>\n",
       "      <td>-0.072425</td>\n",
       "      <td>-0.032806</td>\n",
       "      <td>-0.353924</td>\n",
       "      <td>-0.321489</td>\n",
       "      <td>-0.273905</td>\n",
       "      <td>-0.249959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.193942</td>\n",
       "      <td>0.193297</td>\n",
       "      <td>-0.365775</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>-0.086957</td>\n",
       "      <td>0.375416</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>-0.348806</td>\n",
       "      <td>-1.249487</td>\n",
       "      <td>-1.353039</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.65</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>-0.786256</td>\n",
       "      <td>-0.814815</td>\n",
       "      <td>1.038636</td>\n",
       "      <td>-0.144811</td>\n",
       "      <td>-0.455268</td>\n",
       "      <td>-0.409631</td>\n",
       "      <td>-0.345032</td>\n",
       "      <td>-0.287609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.683801</td>\n",
       "      <td>-0.189385</td>\n",
       "      <td>-0.286728</td>\n",
       "      <td>-1.008621</td>\n",
       "      <td>2.641304</td>\n",
       "      <td>0.366319</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>1.594934</td>\n",
       "      <td>-0.536209</td>\n",
       "      <td>-0.445359</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.70</td>\n",
       "      <td>-0.892857</td>\n",
       "      <td>-0.407230</td>\n",
       "      <td>-0.055556</td>\n",
       "      <td>-0.033269</td>\n",
       "      <td>-0.413921</td>\n",
       "      <td>0.597596</td>\n",
       "      <td>-0.215417</td>\n",
       "      <td>-0.362726</td>\n",
       "      <td>1.101521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-1.145256</td>\n",
       "      <td>-12.192338</td>\n",
       "      <td>0.487585</td>\n",
       "      <td>-0.086207</td>\n",
       "      <td>-0.619565</td>\n",
       "      <td>-0.571779</td>\n",
       "      <td>-0.606061</td>\n",
       "      <td>-0.146792</td>\n",
       "      <td>0.491052</td>\n",
       "      <td>0.463597</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.65</td>\n",
       "      <td>-1.892857</td>\n",
       "      <td>-1.105357</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>-0.745044</td>\n",
       "      <td>0.725620</td>\n",
       "      <td>0.163261</td>\n",
       "      <td>0.592791</td>\n",
       "      <td>0.664445</td>\n",
       "      <td>-0.009402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.793960</td>\n",
       "      <td>-0.048671</td>\n",
       "      <td>1.438290</td>\n",
       "      <td>-0.431034</td>\n",
       "      <td>-0.565217</td>\n",
       "      <td>-1.022888</td>\n",
       "      <td>-1.037879</td>\n",
       "      <td>-0.157624</td>\n",
       "      <td>-0.205613</td>\n",
       "      <td>-0.267403</td>\n",
       "      <td>...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.338837</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.250108</td>\n",
       "      <td>1.204914</td>\n",
       "      <td>0.678458</td>\n",
       "      <td>1.300736</td>\n",
       "      <td>1.408831</td>\n",
       "      <td>0.036058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.516749</td>\n",
       "      <td>-0.420308</td>\n",
       "      <td>-0.596801</td>\n",
       "      <td>0.043103</td>\n",
       "      <td>0.358696</td>\n",
       "      <td>0.622588</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.494090</td>\n",
       "      <td>-0.789968</td>\n",
       "      <td>-0.902805</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>-1.857143</td>\n",
       "      <td>-1.027122</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>-0.926722</td>\n",
       "      <td>-0.463851</td>\n",
       "      <td>-0.249883</td>\n",
       "      <td>-0.533802</td>\n",
       "      <td>-0.646766</td>\n",
       "      <td>0.173933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.757282</td>\n",
       "      <td>-14.775936</td>\n",
       "      <td>-0.196974</td>\n",
       "      <td>0.301724</td>\n",
       "      <td>-0.293478</td>\n",
       "      <td>-0.058358</td>\n",
       "      <td>-0.121212</td>\n",
       "      <td>-0.207010</td>\n",
       "      <td>-1.176422</td>\n",
       "      <td>-1.332077</td>\n",
       "      <td>...</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.935553</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>-0.611735</td>\n",
       "      <td>0.178653</td>\n",
       "      <td>-0.240798</td>\n",
       "      <td>-0.012636</td>\n",
       "      <td>0.156436</td>\n",
       "      <td>-0.081812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows Ã— 493 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0          1         2         3         4         5         6    \\\n",
       "0  -0.437283  11.324702  0.119035 -0.827586 -0.423913 -0.063233 -0.030303   \n",
       "1   0.165243   0.380454  0.166345  0.120690 -0.413043 -0.201605 -0.196970   \n",
       "2  -0.138774  -0.281788 -0.267372  0.491379  0.021739  0.331535  0.318182   \n",
       "3   0.902878   0.702110  3.895225 -0.801724 -0.989130 -1.957763 -2.060606   \n",
       "4   0.531289   0.199302  1.477044 -1.103448 -0.804348 -1.073215 -1.075758   \n",
       "5  -1.099861  -0.686025 -0.788633 -0.189655  3.228261  0.993055  0.886364   \n",
       "6  -0.197197   0.870297  0.606443  0.008621 -0.706522 -0.534772 -0.545455   \n",
       "7   0.591597   0.287134  1.105650 -0.620690  0.978261 -0.842992 -0.992424   \n",
       "8  -0.905857  12.291612 -0.212686 -0.456897  1.728261  0.287679  0.333333   \n",
       "9   0.314776 -14.684118 -1.049204  1.137931  0.771739  1.499147  1.409091   \n",
       "10 -1.676714  -0.064926 -0.780893  0.232759  0.913043  0.913080  0.848485   \n",
       "11 -0.032910   0.036412 -0.100851  0.293103 -0.141304  0.118567  0.075758   \n",
       "12 -1.313436  -0.130102 -0.886674  0.974138  0.500000  1.157298  1.106061   \n",
       "13  0.789213   3.439092 -0.302727  0.439655 -0.195652  0.269697  0.242424   \n",
       "14 -0.193942   0.193297 -0.365775  0.551724 -0.086957  0.375416  0.333333   \n",
       "15 -0.683801  -0.189385 -0.286728 -1.008621  2.641304  0.366319  0.363636   \n",
       "16 -1.145256 -12.192338  0.487585 -0.086207 -0.619565 -0.571779 -0.606061   \n",
       "17  0.793960  -0.048671  1.438290 -0.431034 -0.565217 -1.022888 -1.037879   \n",
       "18 -0.516749  -0.420308 -0.596801  0.043103  0.358696  0.622588  0.757576   \n",
       "19  0.757282 -14.775936 -0.196974  0.301724 -0.293478 -0.058358 -0.121212   \n",
       "\n",
       "         7         8         9    ...   483       484       485       486  \\\n",
       "0   0.000944 -0.634277 -0.592320  ...  0.15  0.678571  0.061820 -0.259259   \n",
       "1  -0.198667 -0.246757 -0.207562  ... -0.95 -0.250000  0.033219 -0.074074   \n",
       "2  -0.259823 -0.389069 -0.292726  ...  0.55 -0.428571  0.275916  0.444444   \n",
       "3   0.051827  2.314668  2.302418  ...  1.35  0.714286  1.037616  1.000000   \n",
       "4   0.096700 -0.596560 -0.785713  ... -0.35 -0.500000  0.421104  0.666667   \n",
       "5   2.266433  0.258509  0.242827  ... -0.40 -1.000000 -0.601387 -0.259259   \n",
       "6  -0.300713 -0.400708 -0.415200  ...  0.10  0.035714  0.688276  1.222222   \n",
       "7   1.043028  0.363843  0.286049  ... -0.20  0.428571  0.801445  1.074074   \n",
       "8   1.714874 -0.013373 -0.069576  ...  0.05  0.250000 -0.239253 -0.259259   \n",
       "9  -0.245192 -0.151860  0.444670  ...  0.40 -1.035714 -0.146556  0.148148   \n",
       "10  0.666502  0.871881  0.880261  ...  0.85 -0.714286  0.188794  0.296296   \n",
       "11 -0.121338  1.488768  1.412093  ... -0.90 -0.678571 -0.779983 -0.500000   \n",
       "12 -0.313414 -0.116011 -0.082338  ...  0.25 -0.178571 -0.051275  0.111111   \n",
       "13 -0.335341  1.139460  1.423966  ...  0.55  0.321429  1.211476  1.259259   \n",
       "14 -0.348806 -1.249487 -1.353039  ... -1.65  0.535714 -0.786256 -0.814815   \n",
       "15  1.594934 -0.536209 -0.445359  ... -1.70 -0.892857 -0.407230 -0.055556   \n",
       "16 -0.146792  0.491052  0.463597  ... -0.65 -1.892857 -1.105357 -0.666667   \n",
       "17 -0.157624 -0.205613 -0.267403  ...  0.10  0.214286  0.338837  0.703704   \n",
       "18  0.494090 -0.789968 -0.902805  ... -0.45 -1.857143 -1.027122 -0.666667   \n",
       "19 -0.207010 -1.176422 -1.332077  ...  1.85  0.178571  0.935553  0.962963   \n",
       "\n",
       "         487       488       489       490       491       492  \n",
       "0   0.902666  0.300817  1.741074  0.038550 -0.012644  0.893020  \n",
       "1   0.615675  0.295694  0.007824  0.138730  0.158757  0.023448  \n",
       "2  -0.416429 -0.218770 -0.418872 -0.371190 -0.303386 -0.199231  \n",
       "3  -0.333900  3.211963  2.412509  3.955306  4.169896  1.088846  \n",
       "4  -0.134101  1.079097  0.658543  1.323777  1.452337  0.297830  \n",
       "5  -0.305168 -1.045606 -0.160233 -0.429439 -0.312671  0.662753  \n",
       "6   0.496591  0.820696  0.185862  0.531539  0.602661 -0.064510  \n",
       "7   0.572222 -0.397379  1.411587  1.301945  1.362036  1.576827  \n",
       "8   0.136953 -0.980137  0.605938  0.053974  0.210664  1.122311  \n",
       "9  -0.561316 -0.916312 -0.906998 -1.126371 -1.054905 -0.393775  \n",
       "10 -0.785849 -0.751357 -0.525453 -0.734687 -0.634623 -0.074486  \n",
       "11 -0.149964  0.026165 -0.289961 -0.178279 -0.084861 -0.200868  \n",
       "12 -0.454634 -0.746975 -0.763399 -0.942918 -0.877118 -0.312454  \n",
       "13 -0.072425 -0.032806 -0.353924 -0.321489 -0.273905 -0.249959  \n",
       "14  1.038636 -0.144811 -0.455268 -0.409631 -0.345032 -0.287609  \n",
       "15 -0.033269 -0.413921  0.597596 -0.215417 -0.362726  1.101521  \n",
       "16 -0.745044  0.725620  0.163261  0.592791  0.664445 -0.009402  \n",
       "17  0.250108  1.204914  0.678458  1.300736  1.408831  0.036058  \n",
       "18 -0.926722 -0.463851 -0.249883 -0.533802 -0.646766  0.173933  \n",
       "19 -0.611735  0.178653 -0.240798 -0.012636  0.156436 -0.081812  \n",
       "\n",
       "[20 rows x 493 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_train_1).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ca4b1f78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.80566406 0.82324219 0.81915934 0.81622678 0.81427175]\n",
      "0.8157128238025416\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_train_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/7n/3dntzndx6kq4bx1z5bsszb880000gn/T/ipykernel_7392/656700193.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m#clf.fit(X_train_raw[:,sf], y_train_raw)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_train_1' is not defined"
     ]
    }
   ],
   "source": [
    "clf = XGBClassifier(max_depth = 11,\n",
    "                    n_estimators = 100,#110\n",
    "                    learning_rate = 0.07,#0.09,\n",
    "                    subsample = 0.8,#0.8,\n",
    "                    colsample_bytree = 0.9,#0.8,\n",
    "                    min_child_weight = 2.0,\n",
    "                    gamma = 0.9,#0.6,\n",
    "                    # reg_alpha = reg_alpha,\n",
    "                    # reg_lambda = reg_lambda,\n",
    "                    # scale_pos_weight = 1,\n",
    "                    # objective = 'multi:softmax',\n",
    "                    # num_class = 4,\n",
    "                    eval_metric = 'auc',\n",
    "                    use_label_encoder=False\n",
    ")\n",
    "\n",
    "#ret = cross_val_score(clf, X_train, y_train_raw, scoring='f1_micro', cv=5)\n",
    "#print(ret)\n",
    "\n",
    "#clf.fit(X_train_2, y_train_raw)\n",
    "#print(np.sort(clf.feature_importances_))\n",
    "#sf = np.argsort(clf.feature_importances_)[::-1][:200]\n",
    "#print(sf)\n",
    "\n",
    "strKFold = StratifiedKFold(n_splits=5,shuffle=False)\n",
    "\n",
    "ret = cross_val_score(clf, X_train_1, y_train_raw, scoring='f1_micro', cv=strKFold)\n",
    "print(ret)\n",
    "print(np.mean(ret))\n",
    "\n",
    "clf.fit(X_train_1, y_train_raw)\n",
    "pred = clf.predict(X_test_1)\n",
    "#clf.fit(X_train_raw[:,sf], y_train_raw)\n",
    "#pred = clf.predict(X_test_raw[:,sf])\n",
    "\n",
    "submission = np.hstack([np.arange(0, len(pred)).reshape(-1,1), pred.reshape(-1,1)]) \n",
    "submission_pd = pd.DataFrame(submission, columns=['id','y'])\n",
    "submission_pd.to_csv('submission.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b3b74ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2,y_train_2= outlier_detection(X_train_1[:,sf], y_train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "dc5b58ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.00101945\n",
      " 0.00112462 0.00112964 0.00113048 0.00115756 0.00120441 0.00121844\n",
      " 0.00123708 0.0012458  0.00125423 0.00125452 0.00125566 0.0012578\n",
      " 0.00126913 0.0012692  0.00127017 0.00127034 0.00127598 0.0012801\n",
      " 0.00128196 0.00128871 0.00128906 0.00129013 0.00129222 0.0012948\n",
      " 0.00130747 0.00131837 0.00132142 0.00132397 0.00133008 0.00133671\n",
      " 0.00134253 0.0013448  0.00134944 0.00135243 0.00135424 0.00135498\n",
      " 0.00135687 0.00135744 0.0013606  0.00136766 0.00137457 0.00138504\n",
      " 0.00139479 0.00139494 0.00140337 0.00140709 0.00140888 0.00140942\n",
      " 0.00141347 0.00141468 0.00141567 0.00141575 0.00141618 0.00142571\n",
      " 0.00142644 0.00143093 0.00143212 0.00143229 0.00143266 0.00144509\n",
      " 0.00144698 0.0014479  0.00144803 0.00144999 0.00145313 0.00145379\n",
      " 0.00145478 0.00145496 0.00145952 0.00145961 0.0014607  0.00146558\n",
      " 0.00146598 0.00146661 0.00146879 0.00147121 0.00147474 0.00147562\n",
      " 0.00147712 0.0014815  0.00148294 0.00148581 0.00148603 0.00149001\n",
      " 0.00149017 0.00149479 0.0014953  0.001497   0.00149754 0.00150215\n",
      " 0.00150245 0.00150554 0.00150893 0.00151315 0.0015132  0.00151747\n",
      " 0.00151976 0.00152202 0.00152952 0.00153139 0.00153221 0.00153308\n",
      " 0.00153741 0.00154123 0.00154182 0.00154196 0.0015439  0.00154502\n",
      " 0.00154815 0.00155448 0.0015557  0.0015599  0.00155992 0.00156566\n",
      " 0.00156669 0.00156788 0.0015744  0.00158044 0.00158067 0.0015808\n",
      " 0.00158095 0.00158231 0.00158246 0.00158271 0.00158328 0.00158516\n",
      " 0.00158602 0.0015884  0.00158915 0.00159031 0.00159189 0.00159264\n",
      " 0.0015944  0.00159656 0.00159966 0.00160037 0.00160118 0.00160192\n",
      " 0.00160441 0.00160625 0.00160756 0.00160838 0.00160887 0.00161028\n",
      " 0.00161101 0.00161524 0.00161586 0.00163546 0.0016393  0.0016396\n",
      " 0.00164078 0.00164253 0.0016445  0.00164519 0.00164626 0.00164823\n",
      " 0.00164841 0.00164974 0.00165095 0.00165131 0.00165199 0.00165274\n",
      " 0.00165349 0.00165441 0.00165453 0.00165505 0.00165549 0.00165695\n",
      " 0.00165791 0.001658   0.00165929 0.00166154 0.00166307 0.00166323\n",
      " 0.00166549 0.00166593 0.00166931 0.00167114 0.00167226 0.00167294\n",
      " 0.00167353 0.00167365 0.0016737  0.0016742  0.00167756 0.00167831\n",
      " 0.00168108 0.00168867 0.00169085 0.00169179 0.001692   0.00169209\n",
      " 0.00169268 0.00169419 0.00169459 0.00169585 0.00169678 0.00169729\n",
      " 0.00170089 0.0017017  0.00170187 0.00170336 0.00170422 0.00170448\n",
      " 0.0017065  0.00170942 0.00170944 0.00171048 0.00171147 0.00171187\n",
      " 0.00171471 0.00171588 0.00171995 0.00172069 0.00172291 0.00172353\n",
      " 0.00172596 0.00172804 0.0017285  0.00173075 0.00173399 0.00173522\n",
      " 0.00173675 0.00174061 0.00174134 0.00174178 0.00174305 0.00174313\n",
      " 0.00174333 0.00174417 0.00174572 0.00174612 0.00175172 0.00176055\n",
      " 0.00176329 0.00176348 0.00176372 0.00176548 0.00176818 0.00176964\n",
      " 0.00177324 0.00177667 0.00178304 0.0017838  0.00178879 0.00178926\n",
      " 0.00178953 0.00178969 0.00179282 0.00180277 0.00180365 0.00180648\n",
      " 0.00180701 0.00180761 0.001811   0.00181108 0.00181643 0.00181794\n",
      " 0.00182773 0.00182854 0.00182858 0.00182916 0.00183224 0.00183231\n",
      " 0.00184111 0.00184169 0.00184248 0.00184292 0.00184323 0.00184369\n",
      " 0.00184541 0.00184637 0.00184785 0.00185108 0.0018537  0.00185506\n",
      " 0.00185637 0.00185923 0.00185954 0.00186053 0.00186174 0.00186523\n",
      " 0.00186565 0.00186849 0.00186972 0.00187279 0.00187287 0.0018747\n",
      " 0.00189526 0.00190031 0.00190075 0.00190151 0.00190345 0.00190427\n",
      " 0.00190615 0.00190668 0.00190767 0.00190768 0.00190775 0.00190859\n",
      " 0.00191392 0.00191532 0.0019233  0.00193002 0.00193058 0.00193291\n",
      " 0.00193721 0.00194103 0.00194355 0.00194713 0.00194799 0.00194862\n",
      " 0.00195269 0.00195427 0.00195451 0.00195675 0.00195876 0.00195957\n",
      " 0.00196205 0.0019635  0.00196817 0.00196855 0.00197328 0.0019736\n",
      " 0.00197783 0.00198006 0.00198382 0.00198536 0.00198607 0.00198768\n",
      " 0.0019941  0.00200116 0.00200176 0.00200361 0.00200728 0.00202199\n",
      " 0.00202333 0.00202464 0.00202639 0.00203127 0.00204137 0.00204831\n",
      " 0.00206039 0.00206084 0.0020612  0.00206387 0.00207402 0.00207456\n",
      " 0.00208427 0.00208952 0.0020925  0.00209458 0.0020962  0.00209958\n",
      " 0.00210262 0.00210929 0.00211127 0.00211558 0.00211747 0.00212253\n",
      " 0.00212339 0.00212719 0.00213087 0.00213795 0.00214854 0.00214946\n",
      " 0.00215461 0.00216427 0.00217533 0.0021757  0.0021774  0.00217993\n",
      " 0.00218212 0.00218317 0.00219056 0.00219129 0.00219719 0.00219825\n",
      " 0.00220316 0.0022066  0.00221589 0.002219   0.00222496 0.00222983\n",
      " 0.00223396 0.00224803 0.00224983 0.00225677 0.00227339 0.00227728\n",
      " 0.00229127 0.00229139 0.00229224 0.0022933  0.00229447 0.00230406\n",
      " 0.00230832 0.00231338 0.00233334 0.00234033 0.00234058 0.00234571\n",
      " 0.00234803 0.00234837 0.00235496 0.00237991 0.00238159 0.00239938\n",
      " 0.00240269 0.00243082 0.00243566 0.00244471 0.00247658 0.00249903\n",
      " 0.0025037  0.00250593 0.00250862 0.00251013 0.00251344 0.00252526\n",
      " 0.00252757 0.00254629 0.00254801 0.00255749 0.00255796 0.00256308\n",
      " 0.00258113 0.00259608 0.00261063 0.00261212 0.0026239  0.00263851\n",
      " 0.0026525  0.00265374 0.00267382 0.00268126 0.00268996 0.00271052\n",
      " 0.002728   0.002739   0.00274666 0.00281061 0.00282455 0.00282614\n",
      " 0.00285741 0.00286179 0.00289447 0.00290547 0.00291886 0.00294531\n",
      " 0.00298821 0.00300448 0.00300623 0.00302865 0.00307112 0.00307538\n",
      " 0.00313754 0.00322553 0.00326193 0.00328896 0.00333346 0.00339714\n",
      " 0.00352325 0.00356915 0.00370734 0.00371352 0.00387281 0.0039375\n",
      " 0.00397189 0.00428897 0.00433136 0.00437503 0.0044716  0.00471415\n",
      " 0.00483577 0.00489713 0.00552209 0.00556255 0.00591254 0.00598578\n",
      " 0.00637682 0.00641077 0.00735432 0.0087368  0.00944405 0.0097862\n",
      " 0.01118597]\n",
      "[302   3 372 491   4   7 284   6 193 111 297 410  90 318 490 296   5  62\n",
      " 283 315 492 412 445 294 440  97 188 110  91  47 330   0 190  37 292 336\n",
      " 194 222  61  48 335  88 388 467 406 285 338 345 331 443 466 432  43 112\n",
      " 481  84  46 489 205 101  44 437 221 367 365  86 407 411  82 401 333 334\n",
      " 306 282 189  40 383 332 270 107 465 477 402 300 247  76 308 344 133 482\n",
      " 414 226  85 317 249 160  70   1 197 114 337 319 359 470 149  29 293 100\n",
      " 418 248  87 339  99 417 343 373  33 394 161 356 444 434 155 379  81 164\n",
      "  19 358 472  77  56 295   2 304 212  53  26 475 108  14 204 361  83  42\n",
      " 136 262 123 174 391 314 371 277 424 454  75 301 303 462 165 322 113  95\n",
      "  54 480  67  65 252 375  28  60 104 346  49  27  55 231 299 228 355 102\n",
      "  89  38  63 488 364 366 342 250 195 311  17 329  80 257 313 448 378 130\n",
      " 398 144  64  66 290 240  98 117  57 211  16 182 305 225 310 260 360 268\n",
      " 279 103 218 141  78 420 156 168 309 181 415 447 162  23  93 323  15 393\n",
      " 236 254 324 399 169 421 116 435 486 422 106 278 463 227 261  39 431   8\n",
      " 203 455 476 229 473 408 446 350 389 298  79 119  32 438 150 256 307  72\n",
      " 281  24 291 142  68  52 230 487 132 213 479 253  50 397 128 430 145 436\n",
      " 453 121 127 125 449 258 280 370 429  58 241 457 264  13 176 185 347 377\n",
      "  96 427  59 357 200 461 458 469 255 276 316 238  51 115  18 374 385 325\n",
      " 354  21 126 442  71 178 269 109  92 288 175 289 353 206 239 152 286 237\n",
      " 134 327 452 208  11  41 272  94 271 232 419 220 224  35 395  25 166 450\n",
      " 148 202 216 263 135 400 426 348 439 201  74 170 187 474 396 464 192 151\n",
      " 484 340 320 199 341 456 158 186 179 409 105 405  10 172 349 363  22 246\n",
      " 351  45  36 153]\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train_1, y_train_raw)\n",
    "print(np.sort(clf.feature_importances_))\n",
    "sf = np.argsort(clf.feature_importances_)[::-1][:400]\n",
    "print(sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "343e44a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train_2, y_train_2)\n",
    "pred = clf.predict(X_test_1[:,sf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1ee9ba4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = np.hstack([np.arange(0, len(pred)).reshape(-1,1), pred.reshape(-1,1)]) \n",
    "submission_pd = pd.DataFrame(submission, columns=['id','y'])\n",
    "submission_pd.to_csv('submission.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6fd9528f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0\n",
       "0   0\n",
       "1   0\n",
       "2   0\n",
       "3   0\n",
       "4   0\n",
       "5   0\n",
       "6   1\n",
       "7   0\n",
       "8   2\n",
       "9   0\n",
       "10  0\n",
       "11  0\n",
       "12  2\n",
       "13  0\n",
       "14  0\n",
       "15  1\n",
       "16  0\n",
       "17  0\n",
       "18  2\n",
       "19  2"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(pred).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6d85cb60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth:7                                                                     \n",
      "n_estimators:95                                                                 \n",
      "learning_rate:0.11                                                              \n",
      "subsample:0.5                                                                   \n",
      "colsample_bytree:0.8999999999999999                                             \n",
      "min_child_weight:2.0                                                            \n",
      "gamma:0.9                                                                       \n",
      "[0.81152344 0.81835938 0.82111437 0.81036168 0.81427175]                        \n",
      "0.8151261226173021                                                              \n",
      "\n",
      "max_depth:7                                                                     \n",
      "n_estimators:95                                                                 \n",
      "learning_rate:0.11                                                              \n",
      "subsample:0.5                                                                   \n",
      "colsample_bytree:0.8999999999999999                                             \n",
      "min_child_weight:2.0                                                            \n",
      "gamma:0.9                                                                       \n",
      "[0.81152344 0.81835938 0.82111437 0.81036168 0.81427175]                        \n",
      "0.8151261226173021                                                              \n",
      "\n",
      "max_depth:9                                                                     \n",
      "n_estimators:95                                                                 \n",
      "learning_rate:0.05                                                              \n",
      "subsample:0.6                                                                   \n",
      "colsample_bytree:0.8999999999999999                                             \n",
      "min_child_weight:4.0                                                            \n",
      "gamma:0.8                                                                       \n",
      "[0.80371094 0.81347656 0.82697947 0.80938416 0.81231672]                        \n",
      "0.8131735703812317                                                              \n",
      "\n",
      "max_depth:11                                                                    \n",
      "n_estimators:100                                                                \n",
      "learning_rate:0.11                                                              \n",
      "subsample:0.7                                                                   \n",
      "colsample_bytree:0.7999999999999999                                             \n",
      "min_child_weight:1.5                                                            \n",
      "gamma:0.5                                                                       \n",
      "[0.80371094 0.82128906 0.82893451 0.81329423 0.8172043 ]                        \n",
      "0.8168866080156404                                                              \n",
      "\n",
      "max_depth:11                                                                    \n",
      "n_estimators:100                                                                \n",
      "learning_rate:0.09                                                              \n",
      "subsample:0.7                                                                   \n",
      "colsample_bytree:0.7999999999999999                                             \n",
      "min_child_weight:1.5                                                            \n",
      "gamma:0.5                                                                       \n",
      "[0.80957031 0.82910156 0.82502444 0.81622678 0.81524927]                        \n",
      "0.8190344727517107                                                              \n",
      "\n",
      "max_depth:10                                                                    \n",
      "n_estimators:110                                                                \n",
      "learning_rate:0.09                                                              \n",
      "subsample:0.8                                                                   \n",
      "colsample_bytree:0.7                                                            \n",
      "min_child_weight:1.5                                                            \n",
      "gamma:0.6                                                                       \n",
      "[0.79882812 0.81738281 0.83088954 0.80938416 0.80742913]                        \n",
      "0.8127827544599219                                                              \n",
      "\n",
      "max_depth:11                                                                    \n",
      "n_estimators:105                                                                \n",
      "learning_rate:0.13                                                              \n",
      "subsample:0.9                                                                   \n",
      "colsample_bytree:0.7999999999999999                                             \n",
      "min_child_weight:1.0                                                            \n",
      "gamma:0.5                                                                       \n",
      "  6%|â–Ž    | 6/100 [14:25<3:46:00, 144.26s/trial, best loss: -0.8190344727517107]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/7n/3dntzndx6kq4bx1z5bsszb880000gn/T/ipykernel_7392/2263126622.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m     }\n\u001b[1;32m     62\u001b[0m \u001b[0malgo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_startup_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGBM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malgo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#max_evals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0;31m# next line is where the fmin is actually executed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                     \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"job exception: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    890\u001b[0m                 \u001b[0mprint_node_on_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrec_eval_print_node_on_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             )\n\u001b[0;32m--> 892\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/7n/3dntzndx6kq4bx1z5bsszb880000gn/T/ipykernel_7392/2263126622.py\u001b[0m in \u001b[0;36mGBM\u001b[0;34m(argsDict)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mstrKFold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mcv_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgbm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrKFold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"f1_micro\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#X_train_1, y_train_1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m     cv_results = cross_validate(\n\u001b[0m\u001b[1;32m    510\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# independent, and that it is pickle-able.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     results = parallel(\n\u001b[0m\u001b[1;32m    268\u001b[0m         delayed(_fit_and_score)(\n\u001b[1;32m    269\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1044\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1398\u001b[0m         )\n\u001b[1;32m   1399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1400\u001b[0;31m         self._Booster = train(\n\u001b[0m\u001b[1;32m   1401\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m             \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1778\u001b[0;31m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0m\u001b[1;32m   1779\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m                                                     dtrain.handle))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from hyperopt import fmin, tpe, hp,space_eval,rand,Trials,partial,STATUS_OK\n",
    "results = []\n",
    "\n",
    "def GBM(argsDict):\n",
    "    max_depth = argsDict[\"max_depth\"] + 7\n",
    "    n_estimators = argsDict['n_estimators'] * 5 + 90\n",
    "    learning_rate = argsDict[\"learning_rate\"] * 0.02 + 0.05\n",
    "    subsample = argsDict[\"subsample\"] * 0.1 + 0.5\n",
    "    colsample_bytree = argsDict[\"colsample_bytree\"] * 0.1 + 0.7\n",
    "    min_child_weight = argsDict[\"min_child_weight\"] * 0.5\n",
    "    gamma = argsDict[\"gamma\"] * 0.1 + 0.5\n",
    "    # reg_alpha = argsDict[\"reg_alpha\"] * 0.1\n",
    "    # reg_lambda = argsDict[\"reg_lambda\"] * 0.1\n",
    "    print(\"max_depth:\" + str(max_depth))\n",
    "    print(\"n_estimators:\" + str(n_estimators))\n",
    "    print(\"learning_rate:\" + str(learning_rate))\n",
    "    print(\"subsample:\" + str(subsample))\n",
    "    print(\"colsample_bytree:\" + str(colsample_bytree))\n",
    "    print(\"min_child_weight:\" + str(min_child_weight))\n",
    "    print(\"gamma:\" + str(gamma))\n",
    "    # print(\"reg_alpha:\" + str(reg_alpha))\n",
    "    # print(\"reg_lambda:\" + str(reg_lambda))\n",
    "\n",
    "    global X_train_1, y_train_1#, X_test\n",
    "\n",
    "    gbm = XGBClassifier(max_depth = max_depth,\n",
    "                        n_estimators = n_estimators,#110\n",
    "                        learning_rate = learning_rate,#0.09,\n",
    "                        subsample = subsample,#0.8,\n",
    "                        colsample_bytree = colsample_bytree,#0.8,\n",
    "                        min_child_weight = min_child_weight,\n",
    "                        gamma = gamma,#0.6,\n",
    "                        # reg_alpha = reg_alpha,\n",
    "                        # reg_lambda = reg_lambda,\n",
    "                        # scale_pos_weight = 1,\n",
    "                        # objective = 'multi:softmax',\n",
    "                        # num_class = 4,\n",
    "                        eval_metric = 'auc',\n",
    "                        use_label_encoder=False\n",
    "    )\n",
    "\n",
    "    strKFold = StratifiedKFold(n_splits=5,shuffle=False)\n",
    "    cv_results = cross_val_score(gbm,X_train_1,y_train_raw,cv=strKFold,scoring=\"f1_micro\") #X_train_1, y_train_1 \n",
    "    metric = cv_results.mean()\n",
    "    results.append(metric)\n",
    "    print(cv_results)\n",
    "    print(str(metric) + '\\n')\n",
    "    return -metric\n",
    "\n",
    "space = {\n",
    "    \"max_depth\":hp.randint(\"max_depth\",5), # + 7\n",
    "    \"n_estimators\":hp.randint(\"n_estimators\",5),  # * 5 + 75\n",
    "    \"learning_rate\":hp.randint(\"learning_rate\",6),  #[0,1,2,3,4,5] -> 0.05,0.06\n",
    "    \"subsample\":hp.randint(\"subsample\",5),#[0,1,2,3] -> [0.7,0.8,0.9]\n",
    "    \"colsample_bytree\":hp.randint(\"colsample_bytree\",3),#[0,1,2,3,4] -> [0.7,0.8,0.9]\n",
    "    \"min_child_weight\":hp.randint(\"min_child_weight\",10), #[0,1,2,3,4,5,6,7] -> +1\n",
    "    \"gamma\":hp.randint(\"gamma\", 5), # * 0.1 + 0.5\n",
    "    # \"reg_alpha\":hp.randint(\"reg_alpha\", 30), # * 0.1\n",
    "    # \"reg_lambda\":hp.randint(\"reg_lambda\", 30), # * 0.1\n",
    "    }\n",
    "algo = partial(tpe.suggest,n_startup_jobs=1)\n",
    "best = fmin(GBM,space,algo=algo,max_evals=100)#max_evals\n",
    "\n",
    "print(best)\n",
    "print(GBM(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e631c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
